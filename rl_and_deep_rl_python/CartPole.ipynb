{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4a7902-8542-4817-b350-8389de8ae333",
   "metadata": {},
   "source": [
    "## DQN Algorithm\n",
    "- Initialize policy network weights\n",
    "- Copy policy network, and call it target network\n",
    "- Initialize replay memory\n",
    "- For each Episode:\n",
    "- - Initialize starting state\n",
    "  - for each time step:\n",
    "  - - select an action via explore/explit\n",
    "    - excute the selected action via gym\n",
    "    - get reward and next state\n",
    "    - store experience in replay memory\n",
    "    - sample random batch from replay memory\n",
    "    - Preprocess states from batch\n",
    "    - Pass batch of preprocessed states to policy network\n",
    "    - Calculate loss between output Q values and target Q values\n",
    "    - Gradeint descent updates weights to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e37088-3bb6-4004-92c6-6b74014f3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c6d18c-1a11-4900-a459-1bdb9c5f1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92090cc7-3a18-4d77-8957-8273436a18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef98b2f6-0f6e-4c31-a32f-d2e9cadbde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3,out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e246d84b-dad0-433a-8d14-f8291120e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=1, action=2, next_state=3, reward=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")\n",
    "e = Experience(1,2,3,4)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e7a64a1-d49e-4488-bbeb-dfa7f76a04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.count = 0\n",
    "\n",
    "    def push(self, exp):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(exp)\n",
    "        else:\n",
    "            self.memory[self.count % self.capacity] = exp\n",
    "            self.count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64560b29-cc7d-482d-a340-693e8f9771f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_explortion_rate(self, step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1.*step*self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf0f3b4-ee71-4456-a9f1-337ce759f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.step)\n",
    "        self.step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c377fa2-5e4d-48e6-ad9f-d7ed382953ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
